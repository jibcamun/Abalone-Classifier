[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "save",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Adam",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "SGD",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "TensorDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Abalone_Classification_Model",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(8, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, 30)\n        )",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "model = Abalone_Classification_Model(8, 30).to(\"cpu\")\nmodel.load_state_dict(torch.load(\"model.pt\"))\nmodel.eval()\nsample = [3,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12]\nX_single = torch.FloatTensor(sample).unsqueeze(0)\nwith torch.no_grad():\n    prediction = model(X_single)\n    predicted_class = torch.argmax(prediction, dim=1).item()\nprint(f\"Predicted Class: {predicted_class + 1}\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "sample = [3,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12]\nX_single = torch.FloatTensor(sample).unsqueeze(0)\nwith torch.no_grad():\n    prediction = model(X_single)\n    predicted_class = torch.argmax(prediction, dim=1).item()\nprint(f\"Predicted Class: {predicted_class + 1}\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "X_single",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "X_single = torch.FloatTensor(sample).unsqueeze(0)\nwith torch.no_grad():\n    prediction = model(X_single)\n    predicted_class = torch.argmax(prediction, dim=1).item()\nprint(f\"Predicted Class: {predicted_class + 1}\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "Abalone_Classification_Model",
        "kind": 6,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "class Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(8, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, y.max() + 1)\n        )",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "data = pd.read_csv(\"data/abalone.csv\")\ndata.dropna()\ndata[\"Sex\"] = pd.factorize(data[\"Sex\"])[0]\nX = data.drop(\"Rings\", axis=1).values\ny = data[\"Rings\"].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_test = torch.FloatTensor(X_train), torch.FloatTensor(X_test)\ny_train, y_test = torch.LongTensor(y_train), torch.LongTensor(y_test)\ndataset = TensorDataset(X_train, y_train)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "data[\"Sex\"]",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "data[\"Sex\"] = pd.factorize(data[\"Sex\"])[0]\nX = data.drop(\"Rings\", axis=1).values\ny = data[\"Rings\"].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_test = torch.FloatTensor(X_train), torch.FloatTensor(X_test)\ny_train, y_test = torch.LongTensor(y_train), torch.LongTensor(y_test)\ndataset = TensorDataset(X_train, y_train)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\nclass Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "X = data.drop(\"Rings\", axis=1).values\ny = data[\"Rings\"].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_test = torch.FloatTensor(X_train), torch.FloatTensor(X_test)\ny_train, y_test = torch.LongTensor(y_train), torch.LongTensor(y_test)\ndataset = TensorDataset(X_train, y_train)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\nclass Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "y = data[\"Rings\"].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_test = torch.FloatTensor(X_train), torch.FloatTensor(X_test)\ny_train, y_test = torch.LongTensor(y_train), torch.LongTensor(y_test)\ndataset = TensorDataset(X_train, y_train)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\nclass Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.model = nn.Sequential(",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "dataset = TensorDataset(X_train, y_train)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\nclass Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(8, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\nclass Abalone_Classification_Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(8, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, y.max() + 1)",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "model = Abalone_Classification_Model(8, y.max() + 1).to(\"cpu\")\noptimizer = SGD(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\nif __name__ == \"__main__\":\n    for epoch in range(1000):\n        for batch in dataloader:\n            X_batch, y_batch = batch\n            X_batch, y_batch = X_batch.to(\"cpu\"), y_batch.to(\"cpu\")\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "optimizer = SGD(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\nif __name__ == \"__main__\":\n    for epoch in range(1000):\n        for batch in dataloader:\n            X_batch, y_batch = batch\n            X_batch, y_batch = X_batch.to(\"cpu\"), y_batch.to(\"cpu\")\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            optimizer.zero_grad()",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "criterion = nn.CrossEntropyLoss()\nif __name__ == \"__main__\":\n    for epoch in range(1000):\n        for batch in dataloader:\n            X_batch, y_batch = batch\n            X_batch, y_batch = X_batch.to(\"cpu\"), y_batch.to(\"cpu\")\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            optimizer.zero_grad()\n            loss.backward()",
        "detail": "train",
        "documentation": {}
    }
]